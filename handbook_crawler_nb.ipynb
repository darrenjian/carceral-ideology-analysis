{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# API access\n",
    "import requests\n",
    "import warnings\n",
    "import json\n",
    "\n",
    "# Reading a website\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import multiprocess\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Progress bar and timing\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"Progress\")\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview: Scraping sites for handbooks\n",
    "\n",
    "We are starting with a list of school records, some of which include websites. There are several steps we need to complete for each school:\n",
    "  1) Obtain the content of the school's website, if it exists\n",
    "  \n",
    "  2) Check whether the content contains any links that match our terms of interest (handbook, conduct, etc.)\n",
    "  \n",
    "  3) Merge the links we found back into the school records\n",
    "  \n",
    "  4) For schools with handbooks, determine whether the link takes us to a webpage or a document we can download, or to multiple documents/webpages\n",
    "  \n",
    "  5) Download handbook documents according to format (html, doc, pdf, google doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the content of the website, if it exists\n",
    "def get_url_content(url):\n",
    "    '''\n",
    "    Takes a url and requests website contents\n",
    "    Accepts: str\n",
    "    Returns: beautiful soup object or str containing error\n",
    "    '''\n",
    "    try:\n",
    "        server_response = requests.get(url, timeout=180)\n",
    "        soup = BeautifulSoup(server_response.content, 'html.parser')\n",
    "    except Exception as e: \n",
    "        soup = e\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the content contains any liks that match our terms of interest\n",
    "def term_search(url, soup, regex):\n",
    "    '''\n",
    "    Returns a list of urls from a webpage (soup) that match search terms\n",
    "    Accepts: url str, soup object (or str), regex str\n",
    "    Returns: list of str\n",
    "    '''\n",
    "    links = []\n",
    "    if isinstance(soup, BeautifulSoup):\n",
    "        # search for matches in anchor text and/or link text\n",
    "        anchor_matches = soup.find_all('a', string=re.compile(regex, re.IGNORECASE))\n",
    "        link_matches = soup.find_all(href=re.compile(regex, re.IGNORECASE))\n",
    "        \n",
    "        # combine into one list without duplicates and convert relative paths\n",
    "        for l in list(set(anchor_matches + link_matches)):\n",
    "            try:\n",
    "                if l['href'].startswith('http'):\n",
    "                    links.append(l['href'])\n",
    "                elif l['href'].startswith('www.'):\n",
    "                    links.append('https://' + l['href'])\n",
    "                elif l['href'].startswith('//www'):\n",
    "                    links.append('https:' + l['href'])\n",
    "                else:\n",
    "                    links.append(url + l['href'])\n",
    "            except Exception as e:\n",
    "                links.append(e) \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the url most likely to be a downloadable handbook\n",
    "def select_url(links):\n",
    "    '''\n",
    "    Return url most likely to be downloadable document\n",
    "    Accepts: list of str\n",
    "    Returns: str\n",
    "    '''\n",
    "    if len(links) == 0:\n",
    "        return 'None'\n",
    "    elif len(links) == 1:\n",
    "        return links[0]\n",
    "    else:\n",
    "        for link in links:\n",
    "            match_pdf = re.search(r'.pdf', link, re.IGNORECASE)\n",
    "            match_doc = re.search(r'.doc', link, re.IGNORECASE)\n",
    "            if match_pdf and len(match_pdf.groups()) == 1:\n",
    "                return match_pdf.groups()[0]\n",
    "            elif match_doc and len(match_doc.groups()) == 1:\n",
    "                return match_doc.groups()[0]\n",
    "            else:\n",
    "                return 'Multiple'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_type(url):\n",
    "    url = str(url)\n",
    "    match_drive = re.search(r'\\.google\\.com', url, re.IGNORECASE)\n",
    "    match_pdf = re.search(r'\\.pdf', url, re.IGNORECASE)\n",
    "    match_doc = re.search(r'\\.doc|\\.rtf', url, re.IGNORECASE)\n",
    "    match_web = re.search(r'http|www\\.', url, re.IGNORECASE)\n",
    "    if match_drive:\n",
    "        return 'gdrive'\n",
    "    elif match_pdf:\n",
    "        return 'pdf'\n",
    "    elif match_doc:\n",
    "        return 'doc'\n",
    "    elif match_web:\n",
    "        return 'web'\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docs(url_list, doc_list = []):\n",
    "    docs = []\n",
    "    \n",
    "    for url in url_list:\n",
    "        doctype = doc_type(url)\n",
    "        if doctype in ['gdrive', 'pdf', 'doc']:\n",
    "            docs.append(url)\n",
    "    return [*docs, *doc_list]\n",
    "            \n",
    "def get_sites(url_list, site_list = []):\n",
    "    sites = []\n",
    "    \n",
    "    for url in url_list:\n",
    "        doctype = doc_type(url)\n",
    "        if doctype == 'web':\n",
    "            sites.append(url)\n",
    "    return [*sites, *site_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_url(url):\n",
    "    if isinstance(url, str):\n",
    "        url_split = re.split(' ', url.rstrip())\n",
    "        if len(url_split) == 1:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handbook_search(df):\n",
    "    '''\n",
    "    Wrapper for term_search() for use with single-argument multiprocessing\n",
    "    Accepts: Pandas dataframe with 'final_website' column\n",
    "    Returns: Pandas dataframe\n",
    "    '''\n",
    "    search_re = r'handbook|conduct'\n",
    "    df['handbooks'] = df.final_website.progress_apply(lambda x: term_search(x, get_url_content(x), search_re))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def handbook_search_recursive(df):\n",
    "#     '''\n",
    "#     Wrapper for term_search() for use with single-argument multiprocessing\n",
    "#     Accepts: Pandas dataframe with 'source_url' column\n",
    "#     Returns: Pandas dataframe\n",
    "#     '''\n",
    "#     search_re = r'handbook|conduct'\n",
    "#     df = df.progress_apply(lambda x: recursive_search(depth=3, doc_list=[], site_list=[x['source_url']], \n",
    "#                                                       search_re=r'handbook|conduct', search_list=[]),\n",
    "#                            axis=1, result_type='expand')\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handbook_search_recursive(df):\n",
    "    '''\n",
    "    Wrapper for term_search() for use with single-argument multiprocessing\n",
    "    Accepts: Pandas dataframe with 'source_url' column\n",
    "    Returns: Pandas dataframe\n",
    "    '''\n",
    "    search = r'handbook|conduct'\n",
    "    data_dict = df.to_dict('records')\n",
    "    for d in tqdm(data_dict):\n",
    "        with open('output.csv', 'a', newline='') as csvfile:\n",
    "            fieldnames = ['docs', 'search_list']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            result = recursive_search(depth=3, doc_list=[], site_list=[d.get('source_url')], \n",
    "                                      search_re=search, search_list=[])\n",
    "            d = d.update(result)\n",
    "            writer.writerow(result)\n",
    "    return pd.DataFrame.from_records(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_search(df, func, n_cores=11):\n",
    "    '''\n",
    "    Wrapper for handbook_search() for use with subsets of a dataframe in parallel\n",
    "    Accepts: Pandas dataframe with 'final_website' column\n",
    "    Returns: Pandas dataframe    \n",
    "    '''\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = multiprocess.Pool(processes=n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_search(depth, doc_list, site_list, search_re, search_list):\n",
    "    if depth == 0 or len(site_list) == 0:\n",
    "        return {'docs': list(set(doc_list)), 'search_list': list(set(search_list))}\n",
    "    else:\n",
    "        new_sites = []\n",
    "        for url in site_list:\n",
    "            if url in search_list:\n",
    "                continue\n",
    "            search_list.append(url)\n",
    "            result = term_search(url, get_url_content(url), search_re)\n",
    "            if len(result) > 0 and len(result) <= 10:\n",
    "                doc_list = get_docs(result, doc_list)\n",
    "                doc_list = [doc for doc in doc_list if is_url(doc)]\n",
    "                new_sites = [*new_sites, *get_sites(result)]\n",
    "        return recursive_search(depth - 1, doc_list, new_sites, search_re, search_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Testing different scenarios\n",
    "sch0 = 'https://www.duneland.k12.in.us/Domain/13' #(href=0, anchor=0)\n",
    "sch1 = 'http://www.springlakeparkschools.org/' #handbook is website, not document\n",
    "sch2 = 'http://www.huntsvillecityschools.org' #(href=0, anchor=1)\n",
    "sch3 = 'http://www.randolphacademy.org' #(href=1, anchor=1)\n",
    "sch4 = 'https://www.floraschools.com/FHS/'\n",
    "sch5 = 'https://www.floraschools.com/fes/handbook.cfm'\n",
    "\n",
    "# Test get_url_content() and term_search()\n",
    "# search_re = r'handbook|conduct'\n",
    "# for school in [sch0, sch1, sch2, sch3, sch4, sch5]:\n",
    "#     results = term_search(school, get_url_content(school), search_re)\n",
    "#     print('Results:', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for school in [sch0, sch1, sch2, sch3, sch4, sch5]:\n",
    "#     print(recursive_search(depth=3, doc_list=[], site_list=[school], \n",
    "#                            search_re=r'handbook|conduct', search_list=[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_urls = ['https://www.floraschools.com/fes/handbook.cfm../cms_files/resources/Elementary Handbook-Student 2021-22.doc', \n",
    "             'https://www.floraschools.com/FHS/fhs-studenthandbook.cfm../cms_files/resources/FHS Student Handbook 2021-2022.pdf', \n",
    "             'https://www.floraschools.com/cms_files/resources/FHS%20Student%20Handbook%202021-2022.pdf']\n",
    "# for t in test_urls:\n",
    "#     print(is_url(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelize handbook search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the school website data from the previous website scraper\n",
    "sites = pd.read_csv('./data/school_websites.csv', dtype=object, usecols=['NCESSCH', 'final_website'])\n",
    "sites = sites.rename({'final_website':'source_url', 'NCESSCH':'school_id'}, axis=1).drop_duplicates().dropna().copy().reset_index(drop=True)\n",
    "\n",
    "sites.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with sample from website data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new columns to dataframe with list of links and final selection of url (or \"Multiple\")\n",
    "# temp = sites.sample(20, random_state=123).reset_index(drop=True)\n",
    "# search_re = r'handbook|conduct|code'\n",
    "# temp['links'] = temp['final_website'].progress_apply(lambda x: term_search(x, get_url_content(x), search_re))\n",
    "# temp['handbooks'] = temp['final_website'].progress_apply(lambda x: select_url(term_search(x, get_url_content(x), search_re)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('output.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = ['docs', 'search_list']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "temp = sites.sample(5, random_state=5).reset_index(drop=True)\n",
    "all_results = handbook_search_recursive(temp)\n",
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = ['docs', 'search_list']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "temp = sites.sample(30, random_state=10).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu = multiprocess.cpu_count() - 1\n",
    "\n",
    "t = time.time()\n",
    "temp = parallel_search(temp, handbook_search_recursive, 10)\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.to_csv('./data/handbook_url_search_results_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.drop(['search_list'], axis=1).explode('docs').to_csv('./data/handbook_url_docs_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application with all school websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = ['docs', 'search_list']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu = multiprocess.cpu_count() - 1\n",
    "\n",
    "t = time.time()\n",
    "docs = parallel_search(sites, handbook_search_recursive, 32)\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.to_csv('./data/handbook_url_search_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.drop(['search_list'], axis=1).explode('docs').to_csv('./data/handbook_url_docs.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
