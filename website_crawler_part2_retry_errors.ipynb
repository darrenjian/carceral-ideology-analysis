{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# API access\n",
    "import requests\n",
    "import warnings\n",
    "\n",
    "# Progress bar and timing\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"Progress\")\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def session_request(url_list):\n",
    "    '''\n",
    "    Retrieves json-formatted web server responses for a list of urls\n",
    "    Accepts: list of str\n",
    "    Returns: list of dict (json objects)\n",
    "    '''\n",
    "    results = []\n",
    "    session = requests.Session()\n",
    "    for url in tqdm(url_list):\n",
    "        response = session.request('GET', url)\n",
    "        results.append(response.json())\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_column(rank1, rank2):\n",
    "    '''\n",
    "    Compares two columns and returns the higher ranked column entry or combination \n",
    "    Accepts: str or null\n",
    "    Returns: str, list of str, or null\n",
    "    '''\n",
    "    if rank1 == rank2:\n",
    "        return rank1\n",
    "    elif pd.notnull(rank1) and pd.notnull(rank2):\n",
    "        return [rank1, rank2]\n",
    "    elif pd.notnull(rank1):\n",
    "        return rank1\n",
    "    else: \n",
    "        return rank2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge school-level source data with errors from Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "school = pd.read_csv('./files/ccd_1819_directory_rev.csv', low_memory=False, dtype=str,\n",
    "                    usecols=['STATENAME', 'NCESSCH', 'SCH_NAME', 'LSTREET1', 'LCITY', 'ST', 'LZIP', 'WEBSITE',\n",
    "                            'SCH_TYPE_TEXT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_id = school[school['SCH_TYPE_TEXT']=='Regular School'].copy().reset_index(drop=True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = pd.read_csv('./files/pid_response_errors.csv').merge(place_id, how='left', left_on='row', right_on='index')\n",
    "errors = errors[errors['status'] != 'OK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors.to_csv('./files/pid_error_details.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retry schools that returned errors with expanded Google Places API query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API key obtained via Google Cloud Console under project gcp-gu-ppalab\n",
    "# local_file = '/Users/sahithi/Documents/school/grad/brodnax/adari_places_auth.txt'\n",
    "local_file = '/Users/nb775/auth/brodnax_places_auth.txt'\n",
    "with open(local_file) as txtfile:\n",
    "    my_key = txtfile.read().strip('\\n')\n",
    "# print(\"API Key: \" + my_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_place = list(errors['SCH_NAME']+'%20'+errors['LSTREET1']+'%20'+errors['LCITY']+'%20'+errors['ST'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the spaces in the search term with '%20' in order to make it compatible with the API\n",
    "place = []\n",
    "for term in g_place:\n",
    "    if isinstance(term, str):\n",
    "        no_space = term.replace(' ', '%20')\n",
    "        place.append(no_space)\n",
    "    else:\n",
    "        place.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the urls for the API \n",
    "pid_url = \"https://maps.googleapis.com/maps/api/place/findplacefromtext/json?input=\"\n",
    "pid_param = '&inputtype=textquery&fields=place_id&key=' + my_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid_api = []\n",
    "for loc in place:\n",
    "    pid_api.append(pid_url + loc + pid_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pid_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid_list = session_request(pid_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get place IDs, keeping the same structure as the original dataframe\n",
    "only_pid = []\n",
    "\n",
    "for result in pid_list:\n",
    "    result_list = result.get('candidates')\n",
    "    if len(result_list) == 1:\n",
    "        only_pid.append(result_list[0].get('place_id'))\n",
    "    elif len(result_list) == 2: # If there are 2 place ids for one school I am wrapping the two place_ids in the following format (place_id 1, place_id 2)\n",
    "        only_pid.append((result_list[0].get('place_id'), result_list[1].get('place_id')))\n",
    "    else:\n",
    "        only_pid.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(only_pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the place_id to the original dataframe\n",
    "errors['g_pid'] = only_pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = errors.explode('g_pid')\n",
    "\n",
    "# Replacing the nan values in 'g_pid' as 'None'\n",
    "errors['g_pid'] = errors['g_pid'].fillna('None')\n",
    "\n",
    "# Resaving the new list of place_ids to only_pid\n",
    "only_pid = list(errors['g_pid'])\n",
    "\n",
    "# Checking the shape to make sure that only the rows with 2 place_ids got duplicated\n",
    "errors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query websites for place IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the urls for the secord part of the API\n",
    "web_url = 'https://maps.googleapis.com/maps/api/place/details/json?place_id='\n",
    "web_param = '&fields=name%2Cwebsite&key=' + my_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a for loop to generate a unique url for each place_id in order to feed that into the API\n",
    "only_pid = list(errors['g_pid'])\n",
    "web_api = []\n",
    "for pid in only_pid:\n",
    "    if pid:\n",
    "        web_api.append(web_url + pid + web_param)\n",
    "    else:\n",
    "        web_api.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_list = session_request(web_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing school names (to ensure match) and websites and adding them to the dataframe\n",
    "places_names = []\n",
    "places_websites = []\n",
    "\n",
    "for escuela in tqdm(web_list):\n",
    "    if (escuela.get('status') == 'OK') == True:\n",
    "        places_names.append(escuela.get('result').get('name'))\n",
    "        places_websites.append(escuela.get('result').get('website'))\n",
    "    else:\n",
    "        places_names.append(None)\n",
    "        places_websites.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors['places_sch'] = places_names\n",
    "errors['places_website'] = places_websites\n",
    "errors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors['final_website'] = errors.progress_apply(lambda x: select_column(x.places_website, x.WEBSITE), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting data for use in Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors.to_csv('./files/school_websites_from_errors.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
